{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook with Optax, Equinox, and JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import equinox as eqx\n",
    "import functools\n",
    "import jaxopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.vmap, in_axes=(None, 0))\n",
    "def network(params, x):\n",
    "  return jnp.dot(params, x)\n",
    "\n",
    "@jax.jit\n",
    "def compute_loss(params, x, y):\n",
    "  y_pred = network(params, x)\n",
    "  loss = jnp.mean(optax.l2_loss(y_pred, y))\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(42)\n",
    "target_params = 0.5\n",
    "\n",
    "# Generate some data.\n",
    "xs = jax.random.normal(key, (16, 2))\n",
    "ys = jnp.sum(xs * target_params, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_learning_rate = 1e-1\n",
    "optimizer = optax.adam(start_learning_rate)\n",
    "\n",
    "# Initialize parameters of the model + optimizer.\n",
    "init_params = jnp.array([0.0, 0.0])\n",
    "opt_state = optimizer.init(init_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3705595135688782\n",
      "Loss: 0.237158864736557\n",
      "Loss: 0.13446101546287537\n"
     ]
    }
   ],
   "source": [
    "# A simple update loop.\n",
    "params = init_params\n",
    "for i in range(3):\n",
    "  loss_value, grads = jax.value_and_grad(compute_loss)(params, xs, ys)\n",
    "  print(f\"Loss: {loss_value}\")\n",
    "  if loss_value < 1e-15:\n",
    "    break\n",
    "  updates, opt_state = optimizer.update(grads, opt_state)\n",
    "  params = optax.apply_updates(params, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.29512683, 0.2951268 ], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce Equinox and a Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(eqx.Module):\n",
    "    xs: jnp.ndarray\n",
    "    ys: jnp.ndarray\n",
    "\n",
    "    def __init__(self, seed: int=42, target_params: float=0.5):\n",
    "        key = jax.random.PRNGKey(seed)\n",
    "        target_params = 0.5\n",
    "\n",
    "        # Generate some data.\n",
    "        self.xs = jax.random.normal(key, (16, 2))\n",
    "        self.ys = jnp.sum(xs * target_params, axis=-1)\n",
    "\n",
    "    def model_network(self, params):\n",
    "        return network(params, self.xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss_function(params, model):\n",
    "  y_pred = model.model_network(params)\n",
    "  y_target = model.ys\n",
    "  loss = jnp.mean(optax.l2_loss(y_pred, y_target))\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.ys.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JAXOPT LBFGSB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: jaxopt.ZoomLineSearch: Iter: 1 Minimum Decrease & Curvature Errors (stop. crit.): 0.0 Stepsize:1.0  Decrease Error:0.0  Curvature Error:0.0 \n",
      "INFO: jaxopt.LBFGS: Iter: 1 Gradient Norm (stop. crit.): 0.7238786816596985 Objective Value:0.14937657117843628  Stepsize:1.0  Number Linesearch Iterations:1 \n",
      "INFO: jaxopt.ZoomLineSearch: Iter: 1 Minimum Decrease & Curvature Errors (stop. crit.): 0.0 Stepsize:1.0  Decrease Error:0.0  Curvature Error:0.0 \n",
      "INFO: jaxopt.LBFGS: Iter: 2 Gradient Norm (stop. crit.): 0.052312254905700684 Objective Value:0.0011559441918507218  Stepsize:1.0  Number Linesearch Iterations:1 \n",
      "INFO: jaxopt.ZoomLineSearch: Iter: 1 Minimum Decrease & Curvature Errors (stop. crit.): 0.0 Stepsize:1.0  Decrease Error:0.0  Curvature Error:0.0 \n",
      "INFO: jaxopt.LBFGS: Iter: 3 Gradient Norm (stop. crit.): 0.014985358342528343 Objective Value:0.00010153277253266424  Stepsize:1.0  Number Linesearch Iterations:1 \n",
      "INFO: jaxopt.ZoomLineSearch: Iter: 1 Minimum Decrease & Curvature Errors (stop. crit.): 0.0 Stepsize:1.0  Decrease Error:0.0  Curvature Error:0.0 \n",
      "INFO: jaxopt.LBFGS: Iter: 4 Gradient Norm (stop. crit.): 2.0062932890141383e-05 Objective Value:1.2456524700610316e-10  Stepsize:1.0  Number Linesearch Iterations:1 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OptStep(params=Array([0.4999901 , 0.49999213], dtype=float32), state=LbfgsState(iter_num=Array(4, dtype=int32, weak_type=True), value=Array(1.2456525e-10, dtype=float32), grad=Array([-1.7826915e-05, -9.2044775e-06], dtype=float32), stepsize=Array(1., dtype=float32), error=Array(2.0062933e-05, dtype=float32), s_history=Array([[ 0.9058708 ,  0.5763672 ],\n",
       "       [-0.38845462, -0.11766708],\n",
       "       [-0.01448965,  0.02802739],\n",
       "       [-0.00293642,  0.01326463],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ]], dtype=float32), y_history=Array([[ 1.6217207 ,  0.68388146],\n",
       "       [-0.6877681 , -0.15165024],\n",
       "       [-0.02373748,  0.02979416],\n",
       "       [-0.00436213,  0.01433262],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ]], dtype=float32), rho_history=Array([5.3670061e-01, 3.5086374e+00, 8.4817627e+02, 4.9279062e+03,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00], dtype=float32), gamma=Array(0.90409404, dtype=float32), aux=None, failed_linesearch=Array(False, dtype=bool), num_fun_eval=Array(9, dtype=int32), num_grad_eval=Array(9, dtype=int32), num_linesearch_iter=Array(4, dtype=int32)))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = init_params\n",
    "solver_2 = jaxopt.LBFGS(fun=loss_function, maxiter = 100, verbose=True)\n",
    "solver_2.run(init_params=init_params, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0: 0.3705595135688782\n",
      "Loss 1: 0.12533925473690033\n",
      "Loss 2: 0.0013298687990754843\n",
      "Loss 3: 0.06173867732286453\n",
      "Loss 4: 0.17377790808677673\n",
      "Loss 5: 0.19612154364585876\n",
      "Loss 6: 0.1262005716562271\n",
      "Loss 7: 0.03880932182073593\n",
      "Loss 8: 0.00013180097448639572\n",
      "Loss 9: 0.02495267614722252\n",
      "Loss 10: 0.07474566251039505\n",
      "Loss 11: 0.09856395423412323\n",
      "Loss 12: 0.07876671850681305\n",
      "Loss 13: 0.03622499853372574\n",
      "Loss 14: 0.004622247070074081\n",
      "Loss 15: 0.0033412498887628317\n",
      "Loss 16: 0.025381486862897873\n",
      "Loss 17: 0.04659378528594971\n",
      "Loss 18: 0.04799724742770195\n",
      "Loss 19: 0.030167732387781143\n",
      "Loss 20: 0.00887465849518776\n",
      "Loss 21: 8.324929012815119e-07\n",
      "Loss 22: 0.0071021090261638165\n",
      "Loss 23: 0.02030024118721485\n",
      "Loss 24: 0.0263848677277565\n",
      "Loss 25: 0.020409852266311646\n",
      "Loss 26: 0.008420398458838463\n",
      "Loss 27: 0.0005593497771769762\n",
      "Loss 28: 0.0019398077856749296\n",
      "Loss 29: 0.00903375819325447\n",
      "Loss 30: 0.013975651003420353\n",
      "Loss 31: 0.012135028839111328\n",
      "Loss 32: 0.005696813575923443\n",
      "Loss 33: 0.0006245278054848313\n",
      "Loss 34: 0.0007463011424988508\n",
      "Loss 35: 0.004612915217876434\n",
      "Loss 36: 0.007606457453221083\n",
      "Loss 37: 0.006707998923957348\n",
      "Loss 38: 0.0030708550475537777\n",
      "Loss 39: 0.00027308432618156075\n",
      "Loss 40: 0.0005436749197542667\n",
      "Loss 41: 0.002829235512763262\n",
      "Loss 42: 0.004314153455197811\n",
      "Loss 43: 0.003452245146036148\n",
      "Loss 44: 0.0012931658420711756\n",
      "Loss 45: 2.544407470850274e-05\n",
      "Loss 46: 0.000600383267737925\n",
      "Loss 47: 0.00195945892482996\n",
      "Loss 48: 0.0024300753138959408\n",
      "Loss 49: 0.0015480443835258484\n",
      "Loss 50: 0.000346560962498188\n",
      "Loss 51: 3.357574314577505e-05\n",
      "Loss 52: 0.0006870031356811523\n",
      "Loss 53: 0.0013502087676897645\n",
      "Loss 54: 0.001224467996507883\n",
      "Loss 55: 0.0005078407702967525\n",
      "Loss 56: 1.73994449141901e-05\n",
      "Loss 57: 0.00019082888320554048\n",
      "Loss 58: 0.0006623659282922745\n",
      "Loss 59: 0.000800388224888593\n",
      "Loss 60: 0.0004602676199283451\n",
      "Loss 61: 6.813267100369558e-05\n",
      "Loss 62: 4.137941141379997e-05\n",
      "Loss 63: 0.00031056831358000636\n",
      "Loss 64: 0.00047908653505146503\n",
      "Loss 65: 0.0003383336297702044\n",
      "Loss 66: 8.088175673037767e-05\n",
      "Loss 67: 6.7552518885349855e-06\n",
      "Loss 68: 0.00014920612738933414\n",
      "Loss 69: 0.00027894083177670836\n",
      "Loss 70: 0.00022330899082589895\n",
      "Loss 71: 6.565159128513187e-05\n",
      "Loss 72: 9.255417126041721e-07\n",
      "Loss 73: 7.803384505677968e-05\n",
      "Loss 74: 0.00016383372712880373\n",
      "Loss 75: 0.0001386395888403058\n",
      "Loss 76: 4.3154672312084585e-05\n",
      "Loss 77: 2.999283310600731e-07\n",
      "Loss 78: 4.614111821865663e-05\n",
      "Loss 79: 9.868330380413681e-05\n",
      "Loss 80: 8.239800808951259e-05\n",
      "Loss 81: 2.3992310161702335e-05\n",
      "Loss 82: 5.38855999820953e-07\n",
      "Loss 83: 3.086286596953869e-05\n",
      "Loss 84: 6.0833353927591816e-05\n",
      "Loss 85: 4.6722678234800696e-05\n",
      "Loss 86: 1.103203248931095e-05\n",
      "Loss 87: 1.3235082860774128e-06\n",
      "Loss 88: 2.2550448193214834e-05\n",
      "Loss 89: 3.768577516893856e-05\n",
      "Loss 90: 2.4701799702597782e-05\n",
      "Loss 91: 3.7494082789635286e-06\n",
      "Loss 92: 2.5068302420550026e-06\n",
      "Loss 93: 1.7073180060833693e-05\n",
      "Loss 94: 2.2736299797543325e-05\n",
      "Loss 95: 1.1604023711697664e-05\n",
      "Loss 96: 6.126589369159774e-07\n",
      "Loss 97: 3.6579083371179877e-06\n",
      "Loss 98: 1.2684155080933124e-05\n",
      "Loss 99: 1.2782886187778786e-05\n",
      "Loss 100: 4.395686119096354e-06\n",
      "Loss 101: 1.4398995773490242e-08\n",
      "Loss 102: 4.303292371332645e-06\n",
      "Loss 103: 8.784206329437438e-06\n",
      "Loss 104: 6.284868959482992e-06\n",
      "Loss 105: 1.043906195263844e-06\n",
      "Loss 106: 5.784594918623043e-07\n",
      "Loss 107: 4.177700247964822e-06\n",
      "Loss 108: 5.372005944082048e-06\n",
      "Loss 109: 2.417846644675592e-06\n",
      "Loss 110: 3.3921040198947594e-08\n",
      "Loss 111: 1.291252146984334e-06\n",
      "Loss 112: 3.3414621611882467e-06\n",
      "Loss 113: 2.694527893254417e-06\n",
      "Loss 114: 5.485443921315891e-07\n",
      "Loss 115: 1.6461275720303092e-07\n",
      "Loss 116: 1.5862076452322071e-06\n",
      "Loss 117: 2.1367354747781064e-06\n",
      "Loss 118: 9.619462844057125e-07\n",
      "Loss 119: 9.73589386887852e-09\n",
      "Loss 120: 5.533798344004026e-07\n",
      "Loss 121: 1.3530473097489448e-06\n",
      "Loss 122: 1.0095428706335952e-06\n",
      "Loss 123: 1.5624914340151008e-07\n",
      "Loss 124: 1.1541985145413491e-07\n",
      "Loss 125: 7.141218247852521e-07\n",
      "Loss 126: 8.183931186067639e-07\n",
      "Loss 127: 2.8267874085941003e-07\n",
      "Loss 128: 3.273285198446274e-09\n",
      "Loss 129: 3.160826054227073e-07\n",
      "Loss 130: 5.62515992896806e-07\n",
      "Loss 131: 3.1368051622848725e-07\n",
      "Loss 132: 1.411469607859317e-08\n",
      "Loss 133: 1.1377032649306784e-07\n",
      "Loss 134: 3.4310966157136136e-07\n",
      "Loss 135: 2.7551737957765e-07\n",
      "Loss 136: 4.547453258396672e-08\n",
      "Loss 137: 2.991617620295983e-08\n",
      "Loss 138: 1.9068534129473846e-07\n",
      "Loss 139: 2.1063939925625164e-07\n",
      "Loss 140: 6.309591782382995e-08\n",
      "Loss 141: 3.804345727331793e-09\n",
      "Loss 142: 9.825419056141982e-08\n",
      "Loss 143: 1.4709161177961505e-07\n",
      "Loss 144: 6.428952303849655e-08\n",
      "Loss 145: 6.239380540007389e-11\n",
      "Loss 146: 4.7505857025953446e-08\n",
      "Loss 147: 9.6699508844722e-08\n",
      "Loss 148: 5.569851424525041e-08\n",
      "Loss 149: 2.2289132761699193e-09\n",
      "Loss 150: 2.1790121706999344e-08\n",
      "Loss 151: 6.116655981713848e-08\n",
      "Loss 152: 4.3732992338618715e-08\n",
      "Loss 153: 4.335954706391476e-09\n",
      "Loss 154: 9.596959671398508e-09\n",
      "Loss 155: 3.785125102240272e-08\n",
      "Loss 156: 3.222789146661853e-08\n",
      "Loss 157: 5.0790056604910205e-09\n",
      "Loss 158: 4.125412900179981e-09\n",
      "Loss 159: 2.325440462414008e-08\n",
      "Loss 160: 2.277997523947306e-08\n",
      "Loss 161: 4.739058034886057e-09\n",
      "Loss 162: 1.7799456353273513e-09\n",
      "Loss 163: 1.4346271726140003e-08\n",
      "Loss 164: 1.566213292392149e-08\n",
      "Loss 165: 3.874447873641884e-09\n",
      "Loss 166: 8.056747491380634e-10\n",
      "Loss 167: 8.978004117921046e-09\n",
      "Loss 168: 1.0580302856055823e-08\n",
      "Loss 169: 2.8876094848584444e-09\n",
      "Loss 170: 4.070161430114183e-10\n",
      "Loss 171: 5.740678155063961e-09\n",
      "Loss 172: 7.059888673666137e-09\n",
      "Loss 173: 2.0058337213413324e-09\n",
      "Loss 174: 2.43222553208966e-10\n",
      "Loss 175: 3.767480993843719e-09\n",
      "Loss 176: 4.665432484785015e-09\n",
      "Loss 177: 1.3064589410305416e-09\n",
      "Loss 178: 1.7626082537969268e-10\n",
      "Loss 179: 2.5374908840802846e-09\n",
      "Loss 180: 3.056962238900951e-09\n",
      "Loss 181: 7.975731741716174e-10\n",
      "Loss 182: 1.5096457417484999e-10\n",
      "Loss 183: 1.7494150572616718e-09\n",
      "Loss 184: 1.9783685800689454e-09\n",
      "Loss 185: 4.5333042875128626e-10\n",
      "Loss 186: 1.4345685750427606e-10\n",
      "Loss 187: 1.225659129744372e-09\n",
      "Loss 188: 1.2603058596738492e-09\n",
      "Loss 189: 2.3224130951682298e-10\n",
      "Loss 190: 1.4217682586803448e-10\n",
      "Loss 191: 8.68282390431574e-10\n",
      "Loss 192: 7.83208831123261e-10\n",
      "Loss 193: 1.0280110790406027e-10\n",
      "Loss 194: 1.4245790658229396e-10\n",
      "Loss 195: 6.144409425701269e-10\n",
      "Loss 196: 4.676999343367072e-10\n",
      "Loss 197: 3.553232708064513e-11\n",
      "Loss 198: 1.3867462733685443e-10\n",
      "Loss 199: 4.2995784710342377e-10\n",
      "Loss 200: 2.6711752254549026e-10\n",
      "Loss 201: 6.589541412527211e-12\n",
      "Loss 202: 1.305643648752408e-10\n",
      "Loss 203: 2.930397313694044e-10\n",
      "Loss 204: 1.4090817401779532e-10\n",
      "Loss 205: 9.174952464441333e-15\n",
      "Loss 206: 1.1694259449690492e-10\n",
      "Loss 207: 1.9275579743460725e-10\n",
      "Loss 208: 6.620887171848722e-11\n",
      "Loss 209: 4.214469051522229e-12\n",
      "Loss 210: 9.977262072080606e-11\n",
      "Loss 211: 1.2057402298815134e-10\n",
      "Loss 212: 2.587291665634428e-11\n",
      "Loss 213: 1.1754161879928837e-11\n",
      "Loss 214: 8.035444531984126e-11\n",
      "Loss 215: 6.989845507954229e-11\n",
      "Loss 216: 6.716434700071439e-12\n",
      "Loss 217: 1.825243428621448e-11\n",
      "Loss 218: 6.076163877599683e-11\n",
      "Loss 219: 3.612675436581725e-11\n",
      "Loss 220: 3.573200763051787e-13\n",
      "Loss 221: 2.1771079730670273e-11\n",
      "Loss 222: 4.134578790448984e-11\n",
      "Loss 223: 1.546416467101963e-11\n",
      "Loss 224: 7.056299988761339e-13\n",
      "Loss 225: 2.1127920593611016e-11\n",
      "Loss 226: 2.550295565006966e-11\n",
      "Loss 227: 4.966346778267905e-12\n",
      "Loss 228: 3.169471629593801e-12\n",
      "Loss 229: 1.8025735418203404e-11\n",
      "Loss 230: 1.388602600960187e-11\n",
      "Loss 231: 6.704376637189924e-13\n",
      "Loss 232: 5.362476088177637e-12\n",
      "Loss 233: 1.3374122989628923e-11\n",
      "Loss 234: 5.995551277671041e-12\n",
      "Loss 235: 6.05696048872062e-14\n",
      "Loss 236: 6.1493379832633366e-12\n",
      "Loss 237: 8.423458111583848e-12\n",
      "Loss 238: 1.7896031878628094e-12\n",
      "Loss 239: 8.984219618257683e-13\n",
      "Loss 240: 5.673635172787073e-12\n",
      "Loss 241: 4.373982079308725e-12\n",
      "Loss 242: 2.0531666644618696e-13\n",
      "Loss 243: 1.8369264442874567e-12\n",
      "Loss 244: 4.279916698823882e-12\n",
      "Loss 245: 1.7418445158456919e-12\n",
      "Loss 246: 6.807228392080589e-14\n",
      "Loss 247: 2.2400987315096543e-12\n",
      "Loss 248: 2.5243002133024106e-12\n",
      "Loss 249: 3.573200763051787e-13\n",
      "Loss 250: 4.722628538234019e-13\n",
      "Loss 251: 1.919036110575867e-12\n",
      "Loss 252: 1.1565887136910646e-12\n",
      "Loss 253: 6.510417205340957e-15\n",
      "Loss 254: 8.289705727415608e-13\n",
      "Loss 255: 1.3337542875691e-12\n",
      "Loss 256: 3.0203617384927384e-13\n",
      "Loss 257: 1.310496849926679e-13\n",
      "Loss 258: 8.511680943401601e-13\n",
      "Loss 259: 6.271719255046548e-13\n",
      "Loss 260: 2.3420501649162873e-14\n",
      "Loss 261: 2.936557247368299e-13\n",
      "Loss 262: 6.290679782638975e-13\n",
      "Loss 263: 2.0531666644618696e-13\n",
      "Loss 264: 2.9145089119886336e-14\n",
      "Loss 265: 3.7457016655029207e-13\n",
      "Loss 266: 3.19973214590874e-13\n",
      "Loss 267: 2.3531523951625388e-14\n",
      "Loss 268: 1.0997452948302566e-13\n",
      "Loss 269: 2.936557247368299e-13\n",
      "Loss 270: 1.28050348102704e-13\n",
      "Loss 271: 1.0354564428105562e-14\n",
      "Loss 272: 1.527181159310942e-13\n",
      "Loss 273: 1.6880420672382712e-13\n",
      "Loss 274: 2.070045523883124e-14\n",
      "Loss 275: 5.701168703797777e-14\n",
      "Loss 276: 1.5941588327184064e-13\n",
      "Loss 277: 5.652596446470426e-14\n",
      "Loss 278: 1.0354564428105562e-14\n",
      "Loss 279: 8.152506447700603e-14\n",
      "Loss 280: 7.380207556195728e-14\n",
      "Loss 281: 4.865899350114944e-15\n",
      "Loss 282: 3.3209546224099995e-14\n",
      "Loss 283: 6.807228392080589e-14\n",
      "Loss 284: 1.814694228219338e-14\n",
      "Loss 285: 4.274358644806853e-15\n",
      "Loss 286: 3.489743216622543e-14\n",
      "Loss 287: 2.9145089119886336e-14\n",
      "Loss 288: 6.123573870198129e-16\n"
     ]
    }
   ],
   "source": [
    "# A simple update loop.\n",
    "params = init_params\n",
    "for i in range(1000):\n",
    "  loss_value, grads = jax.value_and_grad(loss_function)(params, model)\n",
    "  print(f\"Loss {i}: {loss_value}\")\n",
    "  if loss_value < 1e-15:\n",
    "    break\n",
    "  updates, opt_state = optimizer.update(grads, opt_state)\n",
    "  params = optax.apply_updates(params, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quocsproj3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
